

[toc]

# 目标检测学习笔记

目标检测任务的主要目标是：**快速、精准、全面定位；准确分类**.

现阶段目标检测算法可以分为：**Anchor-based；Anchor-free；End-to-End**.

也可以根据检测检测步骤分为：**Two-stage和One-stage**.

**理解：**虽然目标检测算法有很多分类，但是实际上都是为了目标检测任务的目标做的一步步的改进。

Anchor-based算法是目前检测算法的主流。是早期对网络学习能力较差的妥协。但是使用Anchor的目标检测算法，在生成预测框的时候产生了大量的冗余框，需要大量的IoU计算，增加了计算量。并且，anchor是超参数，需要对不同数据集进行统计获得，普适性较差。因此，为了解决这个问题近年提出了一些Anchor-free的目标检测算法。



## 一些思考？

### 单阶段与双阶段算法思考

Faster RCNN代表的双阶段算法在精度上远超单阶段目标检测算法，WHY

1.更均衡的样本？

A-B，A-F都存在一个问题，训练样本不均衡，双阶段算法（先使用RPN网络生产proposal，生成前景背景分类，粗略回归；再使用ROI-RCNN进行分类和精细回归）在第一阶段生成proposal的过程，使样本更均衡（

### Anchor-based和Anchor-free思考

## Detectron2

### 安装

[官网教程链接](https://detectron2.readthedocs.io/en/latest/tutorials/install.html)

1.创建Conda虚拟环境

2.克隆Detectron2库

```shell
git clone https://github.com/facebookresearch/detectron2.git
```

3.安装iopath

```shell
pip install -U 'git+https://github.com/facebookresearch/iopath'
```

4.Build Detectron2 from Source

```sh
python -m pip install -e detectron2
```



## MMDetection

### 安装

[官网教程链接](https://mmdetection.readthedocs.io/en/latest/get_started.html#) 

1.创建Conda虚拟环境

```shell
conda create -n open-mmlab python=3.7 -y
conda activate open-mmlab
```

2.安装根据CUDA(我的环境是cuda10.1)版本安装PyTorch，官方链接：https://pytorch.org/get-started/locally/

```shell
conda install pytorch torchvision torchaudio cudatoolkit=10.1 -c pytorch
```

3.安装mmcv-full

官方推荐使用pip命令安装，在我的docker环境中存在问题，因此我使用下列方法：

```shell
git clone https://github.com/open-mmlab/mmcv.git
cd mmcv
MMCV_WITH_OPS=1 pip install -e .  # package mmcv-full will be installed after this step
cd ..
```

4.克隆MMDetection代码

```shell
git clone https://github.com/open-mmlab/mmdetection.git
cd mmdetection
```

5.安装MMDetection

```shell
pip install -r requirements/build.txt
pip install -v -e .  # or "python setup.py develop"
pip install -r requirements.txt
```

6.测试环境和查看环境信息

查看环境信息

```sh
python mmdet/utils/collect_env.py
```

测试环境

```python
from mmdet.apis import init_detector, inference_detector

config_file = 'configs/faster_rcnn/faster_rcnn_r50_fpn_1x_coco.py'
# download the checkpoint from model zoo and put it in `checkpoints/`
# url: http://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_1x_coco/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth
checkpoint_file = 'checkpoints/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth'
device = 'cuda:0'
# init a detector
model = init_detector(config_file, checkpoint_file, device=device)
# inference the demo image
inference_detector(model, 'demo/demo.jpg')
```

如果没有报错，说明安装成功了！

测试检测单张图

```shell
python demo/image_demo.py \
    ${IMAGE_FILE} \
    ${CONFIG_FILE} \
    ${CHECKPOINT_FILE} \
    [--device ${GPU_ID}] \
    [--score-thr ${SCORE_THR}]
```

Examples: 

```shell
python demo/image_demo.py demo/demo.jpg \
    configs/faster_rcnn/faster_rcnn_r50_fpn_1x_coco.py \
    checkpoints/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth \
    --device cuda:0
```

7.创建checkpoints和data文件夹,文件结构如下

```
mmdetection
├── mmdet
├── tools
├── configs
├── checkpoints
├── data
│   ├── coco
│   │   ├── annotations
│   │   ├── train2017
│   │   ├── val2017
│   │   ├── test2017
│   ├── cityscapes
│   │   ├── annotations
│   │   ├── leftImg8bit
│   │   │   ├── train
│   │   │   ├── val
│   │   ├── gtFine
│   │   │   ├── train
│   │   │   ├── val
│   ├── VOCdevkit
│   │   ├── VOC2007
│   │   ├── VOC2012
```

8.安装mish-cuda

```shell
pip install git+https://github.com/JunnYu/mish-cuda.git 
```

## 基础知识

### 什么是Anchor？

Anchor是在图像上预设好的不同大小，不同长宽比的参照框。Faster RCNN中的RPN使用三个尺度{128，256，512}，三个比例 {1:1, 1:2, 2:1}，共9组anchor。在YOLO中，自v2开始使用anchor。yolo中的anchor是通过k-means算法在数据集中统计得到的，v2中k=5，v3中k=9.

### BN、LN、IN、GN

参考论文：

BN(2015): https://arxiv.org/pdf/1502.03167.pdf

LN(2016): https://arxiv.org/pdf/1607.06450v1.pdf

IN(2017): https://arxiv.org/pdf/1607.08022.pdf

GN(2018): https://arxiv.org/pdf/1803.08494.pdf

#### 一、Batch Normalization

##### Motivation

解决**内部协变量偏移**（Internal Covariate Shift）的问题：深度神经网络的层数较多，每一层的参数更新会导致上层的输入数据分布发生变化，通过层层叠加，高层的输入分布变化会非常剧烈，这就使得高层需要不断去重新适应底层的参数更新。为了训好模型，我们需要非常谨慎地去设定学习率、初始化权重、以及尽可能细致的参数更新策略。

网络一旦train起来，那么参数就要发生更新，除了输入层的数据外(因为输入层数据，我们已经人为的为每个样本归一化)，后面网络每一层的输入数据分布是一直在发生变化的，因为在训练的时候，前面层训练参数的更新将导致后面层输入数据分布的变化。以网络第二层为例：网络的第二层输入，是由第一层的参数和input计算得到的，而第一层的参数在整个训练过程中一直在变化，因此必然会引起后面每一层输入数据分布的改变。

##### 优势和缺陷

加快了模型的收敛速度(允许模型使用更大的学习率)，而且更重要的是在一定程度缓解了深层网络中“梯度弥散”的问题，从而使得训练深层网络模型更加容易和稳定。

**缺陷：**  1. 在batchsize过小时，效果非常差(测试结果表明：batch不能小于32) 2. 不能在RNN中使用(sequence的长度不一致，计算十分繁琐)

##### BN实现

<img src="./img/image-20210204171803918.png" alt="image-20210204171803918" style="zoom: 33%;" />

## 双阶段目标检测算法

## 单阶段目标检测算法

参考文献

v1: [paper](https://arxiv.org/pdf/1506.02640.pdf); v2: [paper](https://arxiv.org/pdf/1612.08242.pdf); v3: [paper](https://pjreddie.com/media/files/papers/YOLOv3.pdf); v4: [paper](https://arxiv.org/pdf/2004.10934.pdf)



### YOLO v1,v2,v3

### Focal Loss

#### Motivation



### YOLO v4

### YOLO v5

v5暂时没有paper只有[代码](https://github.com/ultralytics/yolov5)

#### 代码解读

##### train()

```python
# 加载数据配置文件
with open(opt.data) as f:
    data_dict = yaml.load(f, Loader=yaml.SafeLoader)  # data dict
with torch_distributed_zero_first(rank):
    check_dataset(data_dict)  # check
train_path = data_dict['train']
test_path = data_dict['val']
# 加载类别数和类别名
nc = 1 if opt.single_cls else int(data_dict['nc'])  # number of classes
names = ['item'] if opt.single_cls and len(data_dict['names']) != 1 else data_dict['names']  # class names
assert len(names) == nc, '%g names found for nc=%g dataset in %s' % (len(names), nc, opt.data)  # check
```





### 使用MMDetection实现YOLO v5

#### 前言

使用MMDectection框架实现YOLOv5需要实现：Backbone；Head模块()。

## Anchor Free目标检测算法

### CenterNet

论文链接：[paper](https://arxiv.org/abs/1904.07850) 2019年4月发布

#### Motivation

传统目标检测分为单阶段和双阶段方法。单阶段通过滑动窗口和anchor去做预测，产生大量的冗余proposal，导致严重计算资源浪费

Anchor-based目标检测算法在后处理阶段需要NMS去除多余检测框，算法不是end2end训练的

1.直接预测BBox中心点和尺寸

2.不需要传统NMS操作

#### 原理

输入图片尺寸为：$W \times H \times3$ ，表示为 $I \in R^{W \times H \times 3}$

输出HeatMap，尺寸为：$\frac{W}{R} \times \frac{H}{R}$，通道数$C$为类别数，heatmap中的每个值在$[0,1]$区间，表示为：
$$
Y \in[0,1]^{\frac{W}{R} \times \frac{H}{R} \times C}
$$
$R$ 的默认值为：$R=4$ ，即网络输出的heatmap的$H $和$W $为输入图像的$\frac{1}{4}$

$Y_{x, y, c}$ 表示为heatmap中 $(x, y)$处第$c $通道的值

当$Y_{x, y, z}=1$时，代表heatmap的$(x, y)$处是一个关键点，类别为$c $

当$Y_{x, y, z}=0$时，代表heatmap的$(x, y)$处是背景

需要对label中的bbox坐标进行计算，得到ground truth关键点 $p \in \mathcal{R}^{2}$ 
$$
p=\left(\frac{x_{1}+x_{2}}{2}, \frac{y_{1}+y_{2}}{2}\right)
$$
因为对原始图像进行了$R$ 倍下采样，同理对 $p$ 进行处理：$\tilde{p}=\left\lfloor\frac{p}{R}\right\rfloor$ ，最终得到低分辨率的关键点

关键点使用一个高斯核分布到heatmap中
$$
Y_{x y c}=\exp \left(-\frac{\left(x-\tilde{p}_{x}\right)^{2}+\left(y-\tilde{p}_{y}\right)^{2}}{2 \sigma_{p}^{2}}\right)
$$
其中，$\sigma_{p}$ 与目标的尺寸相关的标准差。如果$(x, y)$处可以从多个同类别目标得到多个值，取**最大值**。

#### 损失函数

CenterNet的损失函数由三部分构成：**关键点损失、中心点偏置损失和目标尺寸损失**

##### 关键点损失

参考Focal Loss，构造如下损失函数：
$$
L_{k}=\frac{-1}{N} \sum_{x y c}\left\{\begin{array}{cl}
\left(1-\hat{Y}_{x y c}\right)^{\alpha} \log \left(\hat{Y}_{x y c}\right) & \text { if } Y_{x y c}=1 \\
\left(1-Y_{x y c}\right)^{\beta}\left(\hat{Y}_{x y c}\right)^{\alpha} \log \left(1-\hat{Y}_{x y c}\right) & \text { otherwise }
\end{array}\right.
$$
$\alpha=2$ 和 $\beta=4$ 是Focal Loss的超参数，$N$ 是图像 $I$ 的的关键点数量，$ \hat{Y}_{x y c}$为预测值，$ Y_{x y c}$为heatmap值

若先将$\left(1-Y_{x y c}\right)^{\beta}$忽略，可将上述公式转化为：
$$
\begin{array}{c}
L_{k}=\left(1-P_{t}\right)^{\alpha} * \log P_{t} \\
P_{t}=\left\{\begin{array}{cc}
\hat{Y}_{x y c}, & \text { if } Y_{x y c}=1 \\
1-\hat{Y}_{x y c}, & \text { otherswise }
\end{array}\right.
\end{array}
$$
$\log P_{t}$为标准**交叉熵损失函数**，$\left(1-\hat{Y}_{x y c}\right)^{\alpha}$ 和 $\left(\hat{Y}_{x y c}\right)^{\alpha}$ 是Focal loss项，存在如下影响：

- 当$Y_{x y c}=1$时，当$ \hat{Y}_{x y c}$值接近1时，由于$\left(1-\hat{Y}_{x y c}\right)^{\alpha}$ 项，损失函数会乘一个很小的系数，急剧衰减；

- 当$Y_{x y c}=1$时，当$ \hat{Y}_{x y c}$值不接近1时，由于$\left(1-\hat{Y}_{x y c}\right)^{\alpha}$ 项，损失函数轻微衰减；
- 当$Y_{x y c} \neq 1$时，由于$\left(\hat{Y}_{x y c}\right)^{\alpha}$，$ \hat{Y}_{x y c}$的值越接近0，权重越小，损失函数所占比重越小
- 当$Y_{x y c} \neq 1$时，由于$\left(1-Y_{x y c}\right)^{\beta}$项，也就是当当前点离目标点越远，$\left(1-Y_{x y c}\right)^{\beta}$越大，反之，当前点离目标点越近，$\left(1-Y_{x y c}\right)^{\beta}$越小，损失函数所占比重越小（平衡正负样本：每个目标只有一个中心点正样本，其余点全是负样本）
- $\left(\hat{Y}_{x y c}\right)^{\alpha}$与$\left(1-Y_{x y c}\right)^{\beta}$协同作用

##### 偏置损失

由于对图像进行了 $R=4$ 的下采样，因此在把特征图映射到原始图像上，会产生精度误差。因此，对于每个中心点，额外增加了一个偏置：$\hat{O} \in \mathcal{R}^{\frac{W}{R} \times \frac{H}{R} \times 2}$ 去补偿。这个偏置值offset用**L1 loss**来训练，公式如下：
$$
L_{o f f}=\frac{1}{N} \sum_{p}\left|\hat{O}_{\tilde{p}}-\left(\frac{p}{R}-\tilde{p}\right)\right|
$$
上式中$\hat{O}_{\tilde{p}}$表示网络预测的offset， $\left(\frac{p}{R}-\tilde{p}\right)$可以根据训练集的标注信息得到。需要特别指出的是，offset损失只针对heatmap中的关键点，对于非关键点，不存在offset损失。

##### 尺寸损失

令$\left(x_{1}^{(k)}, y_{1}^{(k)}, x_{2}^{(k)}, y_{2}^{(k)}\right)$表示第$k$个目标的bbox的左上角和右下角坐标，所属类别为 $c_{k}$,中心点坐标为 $p_{k}=\left(\frac{x_{1}^{(k)}+x_{2}^{(k)}}{2}, \frac{y_{1}^{(k)}+y_{2}^{(k)}}{2}\right)$，然后对每个目标 $k$进行回归，最后回归到 $s_{k}=(x_{2}^{(k)}-x_{1}^{(k)},y_{2}^{(k)}-y_{1}^{(k)})$，这个值是在训练前提前计算得到的，是进行了下采样后的长宽值。采用**L1 loss**训练，公式如下：
$$
L_{\text {size }}=\frac{1}{N} \sum_{k=1}^{N}\left|\hat{S}_{p_{k}}-s_{k}\right|
$$
其中，$\hat{S}_{p_{k}}$为预测值

##### 整体损失

$$
L_{det} = L_{k}+\lambda _{size}L_{size}+\lambda _{offset}L_{offset}
$$

在论文中，$\lambda _{size}=0.1$，$\lambda _{offset}=1$，最后有三个head layer，分别输出$[128, 128, 80], [128, 128, 2], [128, 128, 2]$

#### 推理

1.先对图片进行 $R=4$ 下采样，然后对下采样后的图像进行预测。对于每个类的热点单独提取出来。提取方式如下：

- 采用 $3\times 3 $ Maxpooling的方式对每个点进行判断：当前预测$ \hat{Y}_{x y c}$是否比周围8个临近的的值都大(或者等于)，然后提取 $top=100$个这样的热点

2.生成标定框
$$
(x_{i}+ \delta_{x_{i}}-w_i/2,y_{i}+ \delta_{y_{i}}-h_i/2,x_{i}+ \delta_{x_{i}}+w_i/2,x_{i}+ \delta_{x_{i}}+w_i/2)
$$
其中， $x_i,y_i$是关键点**整型**坐标 ， $\delta_{x_i},\delta_{y_i}$为预测的偏置$offset$， $w_i,h_i$ 为预测目标的长宽

3.阈值，论文中为$0.3$，也就是预测值$ \hat{Y}_{x y c}$ ，从 $top=100$个热点中，输出 $ \hat{Y}_{x y c}>0.3$ 的关键点作为最终结果

#### 缺陷

在实际训练中，如果在图像中，**同一个类别**中的某些物体的GT中心点，在**下采样**时会挤到一块，也就是两个物体在GT中的中心点重叠了，CenterNet对于这种情况也是无能为力的，也就是将这两个物体的当成一个物体来训练(因为只有一个中心点)。同理，在预测过程中，如果两个同类的物体在下采样后的中心点也重叠了，那么CenterNet也是只能检测出一个中心点，不过CenterNet对于这种情况的处理要比faster-rcnn强一些的，具体指标可以查看论文相关部分。

### FCOS：Fully Convolutional One-Stage Object Detection

论文链接：[paper](https://arxiv.org/abs/1904.01355)

代码：[code](https://github.com/tianzhi0549/FCOS/)

#### Motivation

需要仔细调整anchor超参数；

长宽比、大小固定泛化能力差；

产生大量的proposal都被当成负样本；

在为每个grid分配anchor时，需要进行大量且复杂的IoU计算；

#### 方案

与YOLOv1相比，FCOS利用地真边界盒中的所有点来预测边界盒，并通过提出的“中心度”分支来抑制低质量检测到的边界盒。因此，FCOS能够提供类似于我们实验中基于锚的检测器的召回。

##### FCNs

对于位于BBox内的每个grid都当作正样本，否则为负样本。对于 $feature$中的每个 $grid_{x,y}$对应原始 $image$ 中的$\left(\left\lfloor\frac{s}{2}\right\rfloor+x s,\left[\frac{s}{2}\right]+y s\right)$

$s$是 $stride$ ，$BBox$ 回归目标为 $(x,y)$ 到边界的距离 $t_{x,y} =(l^*, t^*, r^*, b^*)$ ,由于回归目标都是正数，为了保证网络输出，使用 $exp()$ 

损失函数为：
$$
L_{total}= L_{Focal} + {\lambda}{\mathbb{1}_{\left\{c_{x, y}^{*}>0\right\}}}L_{IoU}
$$
$\mathbb{1}_{\left\{c_{x, y}^{*}>0\right\}}$ 表示负样本不计算 $IoU$ 损失

**trick：**$IoU$可以替换为新的$IoU$损失，例如$GIoU$, $DIoU$, $CIoU$

##### Multi-level Predic with FPN

采用FPN进行多级预测，$(P_3 ,  P_4, P_5,P_6,P_7)$ 其中 $P_6，P_7$ 由 $P_5$ 使用 $s=2$ 的conv得到，head共享参数。对于每个level可预测的bbox范围进行了限制，（0，64，128，256，512，∞），使用一个可学习的参数 $s_i$ 调整学习到的 $t_{x,y}$ ，将 $exp(t)$，改为了 $exp(s_it)$

#### 要点（important）

##### 样本选择策略

1.限制预测范围，降低混淆样本；

2.选择面积最小的GT分配给正样本；

**trick：选择GT中心点附近的grid作为正样本去做预测，类似yolov5的跨网格预测方案**

##### Center-ness分支

Performance很差：**由于产生大量 $grid_{x,y}$远离预测bbox的中心的低质pred**

**解决方法：**增加一个center-ness分支，衡量预测框中心与 $grid_{x,y}$的一个归一化的系数
$$
\text { centerness }^{*}=\sqrt{\frac{\min \left(l^{*}, r^{*}\right)}{\max \left(l^{*}, r^{*}\right)} \times \frac{\min \left(t^{*}, b^{*}\right)}{\max \left(t^{*}, b^{*}\right)}}
$$
在选择检测框top时，使用 $score \times centerness^*$ 作为最后的得分，降低了$grid_{x,y}$远离center point的pred的分数，再通过nms，剔除低质量检测框。

**trick：**与reg-head共享参数可以提高0.5 AP

### CenterNet2: Probabilistic two-stage detection



## 轻量模型

### ShuffleNetV2

#### 轻量模型设计原则

1.深度可分离卷积：**同等通道大小最小化内存访问量**
$$
MAC=hw(c_1+c_2)+c_1c_2\\
B = hwc_1c_2\\
M A C \geq 2 \sqrt{h w B}+\frac{B}{h w}
$$
所以当 $c_1=c_2$时**MAC**最小

2.分组卷积：**过量使用组卷积会增加MAC**
$$
\begin{aligned}
\mathrm{MAC} &=h w\left(c_{1}+c_{2}\right)+\frac{c_{1} c_{2}}{g} \\
&=h w c_{1}+\frac{B g}{c_{1}}+\frac{B}{h w}
\end{aligned}
$$
可以看出MAC与g正相关，g越大，MAC越大，因此**需要仔细衡量分组数量**

3.多分枝网络：**网络碎片化会降低并行度**

一些网络如Inception，以及Auto ML自动产生的网络NASNET-A，它们倾向于采用“多路”结构，即存在一个lock中很多不同的小卷积或者pooling，这很容易造成网络碎片化，减低模型的并行度，相应速度会慢，这也可以通过实验得到证明。

4.**减少元素级操作：add，ReLU等等**

对于元素级（element-wise operators）比如ReLU和Add，虽然它们的FLOPs较小，但是却需要较大的MAC。这里实验发现如果将ResNet中残差单元中的ReLU和shortcut移除的话，速度有20%的提升。

#### 网络结构设计

<img src="./img/image-20210319153235449.png" alt="image-20210319153235449" style="zoom: 33%;" /><img src="./img/image-20210319153529856.png" alt="image-20210319153529856" style="zoom: 50%;" />

**分析：**

可以看出与shufflenetV1不同，去除了1X1的分组卷积（rule 2），输出输入通道数一致（rule 1），采用Split操作降低通道数，分支合并使用concat（rule4），然后使用shuffle进行信息交流，并且可以和下一个stage的channel split融合为一个元素级操作（rule 4）

对于下采样模块（图D），不再有channel split，而是每个分支都是直接copy一份输入，每个分支都有stride=2的下采样，最后concat在一起后，特征图空间大小减半，但是通道数翻倍。



